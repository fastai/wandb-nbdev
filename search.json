[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Getting Started",
    "section": "",
    "text": "This file will become your README and also the index of your documentation.\nGetting started with experiment tracking? Try the Quickstart →"
  },
  {
    "objectID": "index.html#guides",
    "href": "index.html#guides",
    "title": "Getting Started",
    "section": "Guides",
    "text": "Guides\n\nExperiment Tracking: Visualize experiments in real time\nHyperparameter Tuning: Optimize models quickly\nCollaborative Reports: Describe and share findings\nData + Model Versioning: Version datasets and models\nData Visualization: Visualize predictions across model versions\nIntegrations: PyTorch, Keras, Hugging Face, and more\nPrivate-Hosting: Private cloud and local hosting of the W&B app"
  },
  {
    "objectID": "index.html#reference",
    "href": "index.html#reference",
    "title": "Getting Started",
    "section": "Reference",
    "text": "Reference\n\nPython Library\nCommand Line Interface\nApp UI"
  },
  {
    "objectID": "walkthrough.html",
    "href": "walkthrough.html",
    "title": "Model Management Walkthrough",
    "section": "",
    "text": "In this walkthrough you’ll learn how to use Weights & Biases for Model Management. Track, visualize, and report on the complete production model workflow.\nWe are actively building new Model Management features. Please reach out with questions or suggestions at support@wandb.com.\nNow we will walk through a canonical workflow for producing, organizing, and consuming trained models:\nA companion colab notebook is provided which covers step 2-3 in the first code block and steps 4-6 in the second code block."
  },
  {
    "objectID": "walkthrough.html#create-a-new-registered-model",
    "href": "walkthrough.html#create-a-new-registered-model",
    "title": "Model Management Walkthrough",
    "section": "1. Create a new Registered Model",
    "text": "1. Create a new Registered Model\nFirst, create a Registered Model to hold all the candidate models for your particular modeling task. In this tutorial, we will use the classic MNIST Dataset - 28x28 grayscale input images with output classes from 0-9. The video below demonstrates how to create a new Registered Model.\nUsing Model Registry\n\nVisit your Model Registry at wandb.ai/registry/model (linked from homepage).\n\n\n\n\nClick the Create Registered Model button at the top of the Model Registry.\n\n\n\nMake sure the Owning Entity and Owning Project are set correctly to the values you desire. Enter a unique name for your new Registered Model that describes the modeling task or use-case of interest.\n\n\nUsing Artifact Browser\n\nVisit your Project’s Artifact Browser: wandb.ai/<entity>/<project>/artifacts\nClick the + icon on the bottom of the Artifact Browser Sidebar\nSelect Type: model, Style: Collection, and enter a name. In our case MNIST Grayscale 28x28. Remember, a Collection should map to a modeling task - enter a unique name that describes the use case."
  },
  {
    "objectID": "walkthrough.html#train-log-model-versions",
    "href": "walkthrough.html#train-log-model-versions",
    "title": "Model Management Walkthrough",
    "section": "2. Train & log Model Versions",
    "text": "2. Train & log Model Versions\nNext, you will log a model from your training script:\n\n(Optional) Declare your dataset as a dependency so that it is tracked for reproducibility and audibility\nSerialize your model to disk periodically (and/or at the end of training) using the serialization process provided by your modeling library (eg PyTorch & Keras).\nAdd your model files to an Artifact of type “model”\n\nNote: We use the name f'mnist-nn-{wandb.run.id}'. While not required, it is advisable to name-space your “draft” Artifacts with the Run id in order to stay organized\n\n(Optional) Log training metrics associated with the performance of your model during training.\n\nNote: The data logged immediately before logging your Model Version will automatically be associated with that version\n\nLog your model\n\nNote: If you are logging multiple versions, it is advisable to add an alias of “best” to your Model Version when it outperforms the prior versions. This will make it easy to find the model with peak performance - especially when the tail end of training may overfit!\n\n\nBy default, you should use the native W&B Artifacts API to log your serialized model. However, since this pattern is so common, we have provided a single method which combines serialization, Artifact creation, and logging. See the “(Beta) Using log_model” tab for details.\n\nimport wandb\n\n\nfrom nbdev import show_doc\n\n\n\ninit\n\n init (job_type:Optional[str]=None, dir=None,\n       config:Union[Dict,str,NoneType]=None, project:Optional[str]=None,\n       entity:Optional[str]=None, reinit:bool=None,\n       tags:Optional[Sequence]=None, group:Optional[str]=None,\n       name:Optional[str]=None, notes:Optional[str]=None,\n       magic:Union[dict,str,bool]=None, config_exclude_keys=None,\n       config_include_keys=None, anonymous:Optional[str]=None,\n       mode:Optional[str]=None, allow_val_change:Optional[bool]=None,\n       resume:Union[bool,str,NoneType]=None, force:Optional[bool]=None,\n       tensorboard=None, sync_tensorboard=None, monitor_gym=None,\n       save_code=None, id=None, settings:Union[wandb.sdk.wandb_settings.Se\n       ttings,Dict[str,Any],NoneType]=None)\n\nStarts a new run to track and log to W&B.\nIn an ML training pipeline, you could add wandb.init() to the beginning of your training script as well as your evaluation script, and each piece would be tracked as a run in W&B.\nwandb.init() spawns a new background process to log data to a run, and it also syncs data to wandb.ai by default so you can see live visualizations.\nCall wandb.init() to start a run before logging data with wandb.log(): \nimport wandb\n\nwandb.init()\n# ... calculate metrics, generate media\nwandb.log({\"accuracy\": 0.9})\nwandb.init() returns a run object, and you can also access the run object via wandb.run: \nimport wandb\n\nrun = wandb.init()\n\nassert run is wandb.run\nAt the end of your script, we will automatically call wandb.finish to finalize and cleanup the run. However, if you call wandb.init from a child process, you must explicitly call wandb.finish at the end of the child process.\nFor more on using wandb.init(), including detailed examples, check out our guide and FAQs.\nArguments: project: (str, optional) The name of the project where you’re sending the new run. If the project is not specified, the run is put in an “Uncategorized” project. entity: (str, optional) An entity is a username or team name where you’re sending runs. This entity must exist before you can send runs there, so make sure to create your account or team in the UI before starting to log runs. If you don’t specify an entity, the run will be sent to your default entity, which is usually your username. Change your default entity in your settings under “default location to create new projects”. config: (dict, argparse, absl.flags, str, optional) This sets wandb.config, a dictionary-like object for saving inputs to your job, like hyperparameters for a model or settings for a data preprocessing job. The config will show up in a table in the UI that you can use to group, filter, and sort runs. Keys should not contain . in their names, and values should be under 10 MB. If dict, argparse or absl.flags: will load the key value pairs into the wandb.config object. If str: will look for a yaml file by that name, and load config from that file into the wandb.config object. save_code: (bool, optional) Turn this on to save the main script or notebook to W&B. This is valuable for improving experiment reproducibility and to diff code across experiments in the UI. By default this is off, but you can flip the default behavior to on in your settings page. group: (str, optional) Specify a group to organize individual runs into a larger experiment. For example, you might be doing cross validation, or you might have multiple jobs that train and evaluate a model against different test sets. Group gives you a way to organize runs together into a larger whole, and you can toggle this on and off in the UI. For more details, see our guide to grouping runs. job_type: (str, optional) Specify the type of run, which is useful when you’re grouping runs together into larger experiments using group. For example, you might have multiple jobs in a group, with job types like train and eval. Setting this makes it easy to filter and group similar runs together in the UI so you can compare apples to apples. tags: (list, optional) A list of strings, which will populate the list of tags on this run in the UI. Tags are useful for organizing runs together, or applying temporary labels like “baseline” or “production”. It’s easy to add and remove tags in the UI, or filter down to just runs with a specific tag. name: (str, optional) A short display name for this run, which is how you’ll identify this run in the UI. By default, we generate a random two-word name that lets you easily cross-reference runs from the table to charts. Keeping these run names short makes the chart legends and tables easier to read. If you’re looking for a place to save your hyperparameters, we recommend saving those in config. notes: (str, optional) A longer description of the run, like a -m commit message in git. This helps you remember what you were doing when you ran this run. dir: (str, optional) An absolute path to a directory where metadata will be stored. When you call download() on an artifact, this is the directory where downloaded files will be saved. By default, this is the ./wandb directory. resume: (bool, str, optional) Sets the resuming behavior. Options: \"allow\", \"must\", \"never\", \"auto\" or None. Defaults to None. Cases: - None (default): If the new run has the same ID as a previous run, this run overwrites that data. - \"auto\" (or True): if the previous run on this machine crashed, automatically resume it. Otherwise, start a new run. - \"allow\": if id is set with init(id=\"UNIQUE_ID\") or WANDB_RUN_ID=\"UNIQUE_ID\" and it is identical to a previous run, wandb will automatically resume the run with that id. Otherwise, wandb will start a new run. - \"never\": if id is set with init(id=\"UNIQUE_ID\") or WANDB_RUN_ID=\"UNIQUE_ID\" and it is identical to a previous run, wandb will crash. - \"must\": if id is set with init(id=\"UNIQUE_ID\") or WANDB_RUN_ID=\"UNIQUE_ID\" and it is identical to a previous run, wandb will automatically resume the run with the id. Otherwise wandb will crash. See our guide to resuming runs for more. reinit: (bool, optional) Allow multiple wandb.init() calls in the same process. (default: False) magic: (bool, dict, or str, optional) The bool controls whether we try to auto-instrument your script, capturing basic details of your run without you having to add more wandb code. (default: False) You can also pass a dict, json string, or yaml filename. config_exclude_keys: (list, optional) string keys to exclude from wandb.config. config_include_keys: (list, optional) string keys to include in wandb.config. anonymous: (str, optional) Controls anonymous data logging. Options: - \"never\" (default): requires you to link your W&B account before tracking the run so you don’t accidentally create an anonymous run. - \"allow\": lets a logged-in user track runs with their account, but lets someone who is running the script without a W&B account see the charts in the UI. - \"must\": sends the run to an anonymous account instead of to a signed-up user account. mode: (str, optional) Can be \"online\", \"offline\" or \"disabled\". Defaults to online. allow_val_change: (bool, optional) Whether to allow config values to change after setting the keys once. By default, we throw an exception if a config value is overwritten. If you want to track something like a varying learning rate at multiple times during training, use wandb.log() instead. (default: False in scripts, True in Jupyter) force: (bool, optional) If True, this crashes the script if a user isn’t logged in to W&B. If False, this will let the script run in offline mode if a user isn’t logged in to W&B. (default: False) sync_tensorboard: (bool, optional) Synchronize wandb logs from tensorboard or tensorboardX and save the relevant events file. (default: False) monitor_gym: (bool, optional) Automatically log videos of environment when using OpenAI Gym. (default: False) See our guide to this integration. id: (str, optional) A unique ID for this run, used for resuming. It must be unique in the project, and if you delete a run you can’t reuse the ID. Use the name field for a short descriptive name, or config for saving hyperparameters to compare across runs. The ID cannot contain special characters. See our guide to resuming runs.\nExamples: ### Set where the run is logged\nYou can change where the run is logged, just like changing the organization, repository, and branch in git:\nimport wandb\n\nuser = \"geoff\"\nproject = \"capsules\"\ndisplay_name = \"experiment-2021-10-31\"\n\nwandb.init(entity=user, project=project, name=display_name)\n\n\nAdd metadata about the run to the config\nPass a dictionary-style object as the config keyword argument to add metadata, like hyperparameters, to your run. \nimport wandb\n\nconfig = {\"lr\": 3e-4, \"batch_size\": 32}\nconfig.update({\"architecture\": \"resnet\", \"depth\": 34})\nwandb.init(config=config)\nRaises: Exception: if problem.\nReturns: A Run object.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\njob_type\nOptional\nNone\n\n\n\ndir\nNoneType\nNone\n\n\n\nconfig\nUnion\nNone\n\n\n\nproject\nOptional\nNone\n\n\n\nentity\nOptional\nNone\n\n\n\nreinit\nbool\nNone\n\n\n\ntags\nOptional\nNone\n\n\n\ngroup\nOptional\nNone\n\n\n\nname\nOptional\nNone\n\n\n\nnotes\nOptional\nNone\n\n\n\nmagic\nUnion\nNone\n\n\n\nconfig_exclude_keys\nNoneType\nNone\n\n\n\nconfig_include_keys\nNoneType\nNone\n\n\n\nanonymous\nOptional\nNone\n\n\n\nmode\nOptional\nNone\n\n\n\nallow_val_change\nOptional\nNone\n\n\n\nresume\nUnion\nNone\n\n\n\nforce\nOptional\nNone\n\n\n\ntensorboard\nNoneType\nNone\nalias for sync_tensorboard\n\n\nsync_tensorboard\nNoneType\nNone\n\n\n\nmonitor_gym\nNoneType\nNone\n\n\n\nsave_code\nNoneType\nNone\n\n\n\nid\nNoneType\nNone\n\n\n\nsettings\nUnion\nNone\n\n\n\nReturns\nUnion\n\n\n\n\n\nUsing Artifacts\nimport wandb\n\n# Always initialize a W&B run to start tracking\nwandb.init()\n\n# (Optional) Declare an upstream dataset dependency\n# see the `Declare Dataset Dependency` tab for\n# alternative examples.\ndataset = wandb.use_artifact(\"mnist:latest\")\n\n# At the end of every epoch (or at the end of your script)...\n# ... Serialize your model\nmodel.save(\"path/to/model.pt\")\n# ... Create a Model Version\nart = wandb.Artifact(f'mnist-nn-{wandb.run.id}', type=\"model\")\n# ... Add the serialized files\nart.add_file(\"path/to/model.pt\", \"model.pt\")\n# (optional) Log training metrics\nwandb.log({\"train_loss\": 0.345, \"val_loss\": 0.456})\n# ... Log the Version\nif model_is_best:\n    # If the model is the best model so far, add \"best\" to the aliases\n    wandb.log_artifact(art, aliases=[\"latest\", \"best\"])\nelse:\n    wandb.log_artifact(art)\n(Beta) Using log_model\n\n\n\n\n\n\nWarning\n\n\n\nThe following code snippet leverages actively developed beta APIs and therefore is subject to change and not guaranteed to be backwards compatible.\n\n\nfrom wandb.beta.workflows import log_model\n\n# (Optional) Declare an upstream dataset dependency\n# see the `Declare Dataset Dependency` tab for\n# alternative examples.\ndataset = wandb.use_artifact(\"mnist:latest\")\n\n# (optional) Log training metrics\nwandb.log({\"train_loss\": 0.345, \"val_loss\": 0.456})\n\n# This one method will serialize the model, start a run, create a version\n# add the files to the version, and log the version. You can override\n# the default name, project, aliases, metadata, and more!\nlog_model(model, \"mnist-nn\", aliases=[\"best\"] if model_is_best else [])\nNote: you may want to define custom serialization and deserialization strategies. You can do so by subclassing the _SavedModel class, similar to the _PytorchSavedModel class. All subclasses will automatically be loaded into the serialization registry. As this is a beta feature, please reach out to support@wandb.com with questions or comments.\nDeclare Dataset Dependency\nIf you would like to track your training data, you can declare a dependency by calling wandb.use_artifact on your dataset. Here are 3 examples of how you can declare a dataset dependency:\nDataset stored in W&B\ndataset = wandb.use_artifact(\"[[entity/]project/]name:alias\")\nDataset stored on Local Filesystem\nart = wandb.Artifact(\"dataset_name\", \"dataset\")\nart.add_dir(\"path/to/data\") # or art.add_file(\"path/to/data.csv\")\ndataset = wandb.use_artifact(art)\nDataset stored on Remote Bucket\nart = wandb.Artifact(\"dataset_name\", \"dataset\")\nart.add_reference(\"s3://path/to/data\")\ndataset = wandb.use_artifact(art)\nAfter logging 1 or more Model Versions, you will notice that your will have a new Model Artifact in your Artifact Browser. Here, we can see the results of logging 5 versions to an artifact named mnist_nn-1r9jjogr.\n\nIf you are following along the example notebook, you should see a Run Workspace with charts similar to the image below"
  },
  {
    "objectID": "walkthrough.html#link-model-versions-to-the-registered-model",
    "href": "walkthrough.html#link-model-versions-to-the-registered-model",
    "title": "Model Management Walkthrough",
    "section": "3. Link Model Versions to the Registered Model",
    "text": "3. Link Model Versions to the Registered Model\nNow, let’s say that we are ready to link one of our Model Versions to the Registered Model. We can accomplish this manually as well as via an API.\nManual Linking\nThe following video below demonstrates how to manually link a Model Version to your newly created Registered Model:\n\nNavigate to the Model Version of interest\nClick the link icon\nSelect the target Registered Model\n(optional): Add additional aliases\n\n\nProgramatic Linking\nWhile manual linking is useful for one-off Models, it is often useful to programmatically link Model Versions to a Collection - consider a nightly job or CI pipeline that wants to link the best Model Version from every training job. Depending on your context and use case, you may use one of 3 different linking APIs:\nFetch Model Artifact from Public API:\nimport wandb\n\n# Fetch the Model Version via API\nart = wandb.Api().artifact(...)\n\n# Link the Model Version to the Model Collection\nart.link(\"[[entity/]project/]collectionName\")\nModel Artifact is “used” by the current Run:\nimport wandb\n\n# Initialize a W&B run to start tracking\nwandb.init()\n\n# Obtain a reference to a Model Version\nart = wandb.use_artifact(...)\n\n# Link the Model Version to the Model Collection\nart.link(\"[[entity/]project/]collectionName\")\nModel Artifact is logged by the current Run:\nimport wandb\n\n# Initialize a W&B run to start tracking\nwandb.init()\n\n# Create an Model Version\nart = wandb.Artifact(...)\n\n# Log the Model Version\nwandb.log_artifact(art)\n\n# Link the Model Version to the Collection\nwandb.run.link_artifact(art, \"[[entity/]project/]collectionName\")\n(Beta) Using link_model\nThe following code snippet leverages actively developed beta APIs and therefore is subject to change and not guaranteed to be backwards compatible.\nIn the case that you logged a model with the beta log_model discussed above, then you can use it’s companion method: link_model\nfrom wandb.beta.workflows import log_model, link_model\n\n# Obtain a Model Version\nmodel_version = wb.log_model(model, \"mnist_nn\")\n\n# Link the Model Version\nlink_model(model_version, \"[[entity/]project/]collectionName\")\nAfter you link the Model Version, you will see hyperlinks connecting the Version in the Registered Model to the source Artifact and visa versa."
  },
  {
    "objectID": "walkthrough.html#use-a-model-version",
    "href": "walkthrough.html#use-a-model-version",
    "title": "Model Management Walkthrough",
    "section": "4. Use a Model Version",
    "text": "4. Use a Model Version\nNow we are ready to consume a Model - perhaps to evaluate its performance, make predictions against a dataset, or use in a live production context. Similar to logging a Model, you may choose to use the raw Artifact API or the more opinionated beta APIs.\nUsing Artifacts\nYou can load in a Model Version using the use_artifact method.\nimport wandb\n\n# Always initialize a W&B run to start tracking\nwandb.init()\n\n# Download your Model Version files\npath = wandb.use_artifact(\"[[entity/]project/]collectionName:latest\").download()\n\n# Deserialize your model (this will be your custom code to load in\n# a model from disk - reversing the serialization process used in step 2)\nmodel = make_model_from_data(path)\n(Beta) Using use_model\n\n\n\n\n\n\nWarning\n\n\n\nThe following code snippet leverages actively developed beta APIs and therefore is subject to change and not guaranteed to be backwards compatible.\n\n\nDirectly manipulating model files and handling deserialization can be tricky - especially if you were not the one who serialized the model. As a companion to log_model, use_model automatically deserializes and reconstructs your model for use.\nfrom wandb.beta.workflows import use_model\n\nmodel = use_model(\"[[entity/]project/]collectionName\").model_obj()"
  },
  {
    "objectID": "walkthrough.html#evaluate-model-performance",
    "href": "walkthrough.html#evaluate-model-performance",
    "title": "Model Management Walkthrough",
    "section": "5. Evaluate Model Performance",
    "text": "5. Evaluate Model Performance\nAfter training many Models, you will likely want to evaluate the performance of those models. In most circumstances you will have some held-out data which serves as a test dataset, independent of the dataset your models have access to during training. To evaluate a Model Version, you will want to first complete step 4 above to load a model into memory. Then:\n\n(Optional) Declare a data dependency to your evaluation data\nLog metrics, media, tables, and anything else useful for evaluation\n\n# ... continuation from 4\n\n# (Optional) Declare an upstream evaluation dataset dependency\ndataset = wandb.use_artifact(\"mnist-evaluation:latest\")\n\n# Evaluate your model in whatever way makes sense for your\nloss, accuracy, predictions = evaluate_model(model, dataset)\n\n# Log out metrics, images, tables, or any data useful for evaluation.\nwandb.log({\"loss\": loss, \"accuracy\": accuracy, \"predictions\": predictions})\nIf you are executing similar code, as demonstrated in the notebook, you should see a workspace similar to the image below - here we even show model predictions against the test data!"
  },
  {
    "objectID": "walkthrough.html#promote-a-version-to-production",
    "href": "walkthrough.html#promote-a-version-to-production",
    "title": "Model Management Walkthrough",
    "section": "6. Promote a Version to Production",
    "text": "6. Promote a Version to Production\nNext, you will likely want to denote which version in the Registered Model is intended to be used for Production. Here, we use the concept of aliases. Each Registered Model can have any aliases which make sense for your use case - however we often see production as the most common alias. Each alias can only be assigned to a single Version at a time.\nvia UI Interface\n\nvia API\nFollow steps in Part 3. Link Model Versions to the Collection and add the aliases you want to the aliases parameter.\nThe image below shows the new production alias added to v1 of the Registered Model!"
  },
  {
    "objectID": "walkthrough.html#consume-the-production-model",
    "href": "walkthrough.html#consume-the-production-model",
    "title": "Model Management Walkthrough",
    "section": "7. Consume the Production Model",
    "text": "7. Consume the Production Model\nFinally, you will likely want to use your production Model for inference. To do so, simply follow the steps outlined in Part 4. Using a Model Version, with the production alias. For example:\nwandb.use_artifact(\"[[entity/]project/]registeredModelName:production\")\nYou can reference a Version within the Registered Model using different alias strategies:\n\nlatest - which will fetch the most recently linked Version\nv# - using v0, v1, v2, … you can fetch a specific version in the Registered Model\nproduction - you can use any custom alias that you and your team have assigned"
  },
  {
    "objectID": "walkthrough.html#build-a-reporting-dashboard",
    "href": "walkthrough.html#build-a-reporting-dashboard",
    "title": "Model Management Walkthrough",
    "section": "8. Build a Reporting Dashboard",
    "text": "8. Build a Reporting Dashboard\nUsing Weave Panels, you can display any of the Model Registry/Artifact views inside of Reports! See a demo here. Below is a full-page screenshot of an example Model Dashboard.\n\n\n\nfoo\n\n foo ()"
  }
]